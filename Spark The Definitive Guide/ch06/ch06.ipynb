{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e78e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6c1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f0c1139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/12 07:08:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/12 07:08:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/12 07:08:44 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"yarn\") \\\n",
    "        .appName(\"ch06\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f78e0139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kumar-rke2-1:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ch06</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=ch06>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f28278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   2 hadoop supergroup     275001 2025-05-10 14:57 /data/retail-data/by-day/2010-12-01.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /data/retail-data/by-day/2010-12-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17345184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"hdfs:///data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbbd5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sql table\n",
    "df.createOrReplaceTempView(\"retail_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d090ace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827f6ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('InvoiceNo', StringType(), True), StructField('StockCode', StringType(), True), StructField('Description', StringType(), True), StructField('Quantity', IntegerType(), True), StructField('InvoiceDate', TimestampType(), True), StructField('UnitPrice', DoubleType(), True), StructField('CustomerID', DoubleType(), True), StructField('Country', StringType(), True)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69efc821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924161b",
   "metadata": {},
   "source": [
    "#### Converting to Spark Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00048327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "356c1749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  1|one|\n",
      "+---+---+\n",
      "|  1|one|\n",
      "|  1|one|\n",
      "+---+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(1), lit(\"one\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf768172",
   "metadata": {},
   "source": [
    "#### Working With Boolens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d81849d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------+\n",
      "|InvoiceNo|Description                       |\n",
      "+---------+----------------------------------+\n",
      "|536365   |WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|536365   |WHITE METAL LANTERN               |\n",
      "+---------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo != 536563\").select(\"InvoiceNo\", \"Description\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1d47e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|InvoiceNo|         Description|\n",
      "+---------+--------------------+\n",
      "|   536365|WHITE HANGING HEA...|\n",
      "|   536365| WHITE METAL LANTERN|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select InvoiceNo, Description\n",
    "    from retail_data\n",
    "    where InvoiceNo != 536563\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8aab5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr, col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed31ee9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|InvoiceNo|UnitPrice|Description|\n",
      "+---------+---------+-----------+\n",
      "|536370   |18.0     |POSTAGE    |\n",
      "|536403   |15.0     |POSTAGE    |\n",
      "|536527   |18.0     |POSTAGE    |\n",
      "+---------+---------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priceFilter = col(\"UnitPrice\") >  600\n",
    "descFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "\n",
    "df.where(priceFilter | descFilter) \\\n",
    "    .select(\"InvoiceNo\", \"UnitPrice\", \"Description\") \\\n",
    "    .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "230e5a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|InvoiceNo|UnitPrice|Description|\n",
      "+---------+---------+-----------+\n",
      "|536370   |18.0     |POSTAGE    |\n",
      "|536403   |15.0     |POSTAGE    |\n",
      "|536527   |18.0     |POSTAGE    |\n",
      "+---------+---------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select InvoiceNo, UnitPrice, Description\n",
    "    from retail_data\n",
    "    where UnitPrice > 600 or instr(Description, \"POSTAGE\") >=1\n",
    "\"\"\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e185cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|UnitPrice|isexpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"isexpensive\", (col(\"StockCode\")==\"DOT\") & (priceFilter | descFilter)) \\\n",
    ".where(\"isexpensive\").select(\"UnitPrice\", \"isexpensive\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "292253bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isexpensive|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|       true|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|       true|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    with isexpensive as (\n",
    "          select *, (StockCode=\"DOT\") AND ((UnitPrice > 600) or (Description like '%POSTAGE%')) as isexpensive\n",
    "          from retail_data\n",
    "    )\n",
    "    select *\n",
    "    from isexpensive\n",
    "    where isexpensive\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d12b88",
   "metadata": {},
   "source": [
    "#### Working With Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4ae4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pow, round, bround, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dcf99aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fabricatedQuantity = (pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5).alias(\"realQuantity\")\n",
    "df.select(\"CustomerID\", fabricatedQuantity).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7b8cff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select CustomerID, pow(Quantity * UnitPrice, 2) + 5 as realQuantity\n",
    "    from retail_data \n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "691b6793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+--------------------+--------------+--------------+\n",
      "|UnitPrice|round(UnitPrice, 0)|round(UnitPrice, 2)|bround(UnitPrice, 0)|bround(2.5, 0)|bround(2.6, 0)|\n",
      "+---------+-------------------+-------------------+--------------------+--------------+--------------+\n",
      "|     2.55|                3.0|               2.55|                 3.0|           2.0|           3.0|\n",
      "|     3.39|                3.0|               3.39|                 3.0|           2.0|           3.0|\n",
      "+---------+-------------------+-------------------+--------------------+--------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"UnitPrice\", round(\"UnitPrice\"), round(\"UnitPrice\", 2), bround(\"UnitPrice\"), bround(lit(2.5)), bround(lit(2.6))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "087b905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|round(UnitPrice, 0)|round(UnitPrice, 2)|bround(UnitPrice, 0)|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|                3.0|               2.55|                 3.0|\n",
      "|                3.0|               3.39|                 3.0|\n",
      "+-------------------+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select round(UnitPrice), round(UnitPrice, 2), bround(UnitPrice)\n",
    "    from retail_data\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62263bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perarson Correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33d1ac0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04112314436835552"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr(\"UnitPrice\", \"Quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "19d143a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(UnitPrice, Quantity)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835552|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr(\"UnitPrice\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6501b49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                NULL| 8.627413127413128| 4.151946589446603|15661.388719512195|          NULL|\n",
      "| stddev|72.89447869788873|17407.897548583845|                NULL|26.371821677029203|15.638659854603892|1854.4496996893627|          NULL|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d19ecddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|sl_no|InvoiceNo|\n",
      "+-----+---------+\n",
      "|    0|   536365|\n",
      "|    1|   536365|\n",
      "|    2|   536365|\n",
      "|    3|   536365|\n",
      "|    4|   536365|\n",
      "+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df.select(monotonically_increasing_id().alias(\"sl_no\"), \"InvoiceNo\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d3c89a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 11:13:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/12 11:13:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/12 11:13:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|row_no|CustomerID|\n",
      "+------+----------+\n",
      "|     1|   18229.0|\n",
      "|     2|   18229.0|\n",
      "|     3|   18229.0|\n",
      "|     4|   18229.0|\n",
      "|     5|   18229.0|\n",
      "+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select row_number() over(order by CustomerID desc) as row_no, CustomerID\n",
    "    from retail_data\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ed649",
   "metadata": {},
   "source": [
    "#### Working With Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f8f16092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap, lower, upper\n",
    "from pyspark.sql.functions import trim, ltrim, rtrim, lpad, rpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3a6fba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------+----------------+----------------+\n",
      "|CustomerID|initcap(Description)              |lower(StockCode)|upper(StockCode)|\n",
      "+----------+----------------------------------+----------------+----------------+\n",
      "|17850.0   |White Hanging Heart T-light Holder|85123a          |85123A          |\n",
      "|17850.0   |White Metal Lantern               |71053           |71053           |\n",
      "+----------+----------------------------------+----------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"CustomerID\", initcap(\"Description\"), lower(\"StockCode\"), upper(\"StockCode\"), ).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c3976595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+---------------------------+---------------------------+------------------+------------------+\n",
      "|trim(     Hello  coco    )|ltrim(     Hello  coco    )|rtrim(     Hello  coco    )|lpad(HELLO, 10, *)|rpad(HELLO, 10, *)|\n",
      "+--------------------------+---------------------------+---------------------------+------------------+------------------+\n",
      "|               Hello  coco|            Hello  coco    |                Hello  coco|        *****HELLO|        HELLO*****|\n",
      "|               Hello  coco|            Hello  coco    |                Hello  coco|        *****HELLO|        HELLO*****|\n",
      "+--------------------------+---------------------------+---------------------------+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    trim(lit(\"     Hello  coco    \")),\n",
    "    ltrim(lit(\"     Hello  coco    \")),\n",
    "    rtrim(lit(\"     Hello  coco    \")),\n",
    "    lpad(lit(\"HELLO\"), 10, \"*\"),\n",
    "    rpad(lit(\"HELLO\"), 10 ,\"*\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b9289b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+---------------------------+---------------------------+------------------+------------------+\n",
      "|trim(     Hello  coco    )|ltrim(     Hello  coco    )|rtrim(     Hello  coco    )|lpad(HELLO, 10, *)|rpad(HELLO, 10, *)|\n",
      "+--------------------------+---------------------------+---------------------------+------------------+------------------+\n",
      "|               Hello  coco|            Hello  coco    |                Hello  coco|        *****HELLO|        HELLO*****|\n",
      "+--------------------------+---------------------------+---------------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        trim(\"     Hello  coco    \"),\n",
    "        ltrim(\"     Hello  coco    \"),\n",
    "        rtrim(\"     Hello  coco    \"),\n",
    "        lpad(\"HELLO\", 10, \"*\"),\n",
    "        rpad(\"HELLO\", 10 ,\"*\")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341b4d7",
   "metadata": {},
   "source": [
    "#### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "78b1d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, regexp_extract, regexp_substr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea9379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------+\n",
      "|CustomerID|Description                       |\n",
      "+----------+----------------------------------+\n",
      "|17850.0   |COLOR HANGING HEART T-LIGHT HOLDER|\n",
      "|17850.0   |COLOR METAL LANTERN               |\n",
      "+----------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"CustomerID\", regexp_replace(\"Description\", \"BLACK|WHITE|RED|BLUE|GREEN\", \"COLOR\").alias(\"Description\")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523db081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------+\n",
      "|CustomerID|Description                       |\n",
      "+----------+----------------------------------+\n",
      "|17850.0   |COLOR HANGING HEART T-LIGHT HOLDER|\n",
      "|17850.0   |COLOR METAL LANTERN               |\n",
      "+----------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select CustomerID, regexp_replace(Description, \"BLACK|WHITE|RED|BLUE|GREEN\", \"COLOR\") as Description\n",
    "    from retail_data\n",
    "\"\"\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14f5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|Description                       |translate(Description, LET, *$@)  |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|WHI@$ HANGING H$AR@ @-*IGH@ HO*D$R|\n",
      "|WHITE METAL LANTERN               |WHI@$ M$@A* *AN@$RN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# character level replacement\n",
    "from pyspark.sql.functions import translate\n",
    "df.select(\"Description\", translate(\"Description\", \"LET\", \"*$@\")).show(2, False)\n",
    "\n",
    "# here L -> *, E -> $ , T-> @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f8860823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|Description                       |translate(Description, LET, *$@)  |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|WHI@$ HANGING H$AR@ @-*IGH@ HO*D$R|\n",
      "|WHITE METAL LANTERN               |WHI@$ M$@A* *AN@$RN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select Description, translate(Description, \"LET\", \"*$@\")\n",
    "    from retail_data \n",
    "\"\"\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3c8deab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----+\n",
      "|Description                        |color|\n",
      "+-----------------------------------+-----+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |WHITE|\n",
      "|WHITE METAL LANTERN                |WHITE|\n",
      "|CREAM CUPID HEARTS COAT HANGER     |     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|     |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |RED  |\n",
      "+-----------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#regex_extract\n",
    "\n",
    "df.select(\"Description\", regexp_extract(\"Description\", \"(BLACK|WHITE|RED|BLUE|GREEN)\", 1).alias(\"color\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "63d22e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----+\n",
      "|Description                        |color|\n",
      "+-----------------------------------+-----+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |WHITE|\n",
      "|WHITE METAL LANTERN                |WHITE|\n",
      "|CREAM CUPID HEARTS COAT HANGER     |NULL |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|NULL |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |RED  |\n",
      "+-----------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#regex_substr\n",
    "df.select(\"Description\", regexp_substr(\"Description\", lit(\"BLACK|WHITE|RED|BLUE|GREEN\")).alias(\"color\")).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f1b35",
   "metadata": {},
   "source": [
    "#### Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "93016e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, unix_timestamp\n",
    "from pyspark.sql.functions import date_diff, date_add, date_sub, now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "03f8976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------------+\n",
      "|id |today     |now                       |\n",
      "+---+----------+--------------------------+\n",
      "|0  |2025-05-12|2025-05-12 14:55:54.764549|\n",
      "+---+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datedf = spark.range(1).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp())\n",
    "datedf.createOrReplaceTempView(\"date_table\")\n",
    "datedf.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41590b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7c1ca0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+\n",
      "|current_date()|current_timestamp()       |\n",
      "+--------------+--------------------------+\n",
      "|2025-05-12    |2025-05-12 14:56:04.641913|\n",
      "+--------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select current_date(), current_timestamp()\n",
    "\"\"\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2980bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 2)|date_add(today, 2)|\n",
      "+------------------+------------------+\n",
      "|        2025-05-10|        2025-05-14|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datedf.select(date_sub(\"today\", 2), date_add(\"today\", 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "825d6827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------------------------------------------------+----------------------------------------------------------+\n",
      "|date_sub(today, 2)|date_add(today, 2)|date_add(today, (- extractansiintervaldays(INTERVAL '2' DAY)))|date_add(today, extractansiintervaldays(INTERVAL '2' DAY))|\n",
      "+------------------+------------------+--------------------------------------------------------------+----------------------------------------------------------+\n",
      "|        2025-05-10|        2025-05-14|                                                    2025-05-10|                                                2025-05-14|\n",
      "+------------------+------------------+--------------------------------------------------------------+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select date_sub(today, 2), date_add(today, 2), today - INTERVAL 2 DAY, today + INTERVAL 2 DAY\n",
    "    from date_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50258b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, to_date, to_timestamp, months_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "14cfa7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------------+\n",
      "|months_between(end, start, true)|date_diff(end, start)|\n",
      "+--------------------------------+---------------------+\n",
      "|                      3.19354839|                   95|\n",
      "+--------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datedf.select(to_date(lit(\"2025-02-05\")).alias(\"start\"), \n",
    "            to_date(lit(\"2025-05-11\")).alias(\"end\")) \\\n",
    "            .select(months_between(\"end\", \"start\"), date_diff(\"end\", \"start\")).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f7a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|to_date(2025-23-05)|to_date(2025-05-23)|\n",
      "+-------------------+-------------------+\n",
      "|               NULL|         2025-05-23|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to_date expects date format yyyy-mm-dd if not then ti return NULL\n",
    "datedf.select(to_date(lit(\"2025-23-05\")), to_date(lit(\"2025-05-23\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a7b0530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|to_date(2015-24-05, yyyy-dd-MM)|\n",
      "+-------------------------------+\n",
      "|                     2015-05-24|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to fix one need to pass the format\n",
    "datedf.select(to_date(lit(\"2015-24-05\"), \"yyyy-dd-MM\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f79aea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------------------+\n",
      "|to_date(2025-04-23)|to_date(2025-23-04)|to_date(2025-23-04, yyyy-dd-MM)|\n",
      "+-------------------+-------------------+-------------------------------+\n",
      "|         2025-04-23|               NULL|                     2025-04-23|\n",
      "+-------------------+-------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select TO_DATE(\"2025-04-23\"), TO_DATE(\"2025-23-04\"), TO_DATE(\"2025-23-04\", \"yyyy-dd-MM\")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc749c",
   "metadata": {},
   "source": [
    "#### Working With NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0a1b5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, ifnull, nullif, nvl, nvl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c60246e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------------+------------+------------+----------------+--------------------------+-------------------------+\n",
      "|coalesce(NULL, NULL, 2.5)|ifnull(NULL, 24)|nullif(2, 2)|nullif(2, 3)|nvl(NULL, hello)|nvl2(first, second, third)|nvl2(NULL, second, third)|\n",
      "+-------------------------+----------------+------------+------------+----------------+--------------------------+-------------------------+\n",
      "|                      2.5|              24|        NULL|           2|           hello|                    second|                    third|\n",
      "+-------------------------+----------------+------------+------------+----------------+--------------------------+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(coalesce(lit(None),  lit(None), lit(2.5)),\n",
    "          ifnull(lit(None), lit(24)),\n",
    "          nullif(lit(2), lit(2)),\n",
    "          nullif(lit(2), lit(3)),\n",
    "          nvl(lit(None), lit(\"hello\")),\n",
    "          nvl2(lit(\"first\"), lit(\"second\"), lit(\"third\")),\n",
    "          nvl2(lit(None), lit(\"second\"), lit(\"third\"))\n",
    "    ).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "abd18487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------------+------------+------------+----------------+--------------------------+-------------------------+\n",
      "|coalesce(NULL, NULL, 4, NULL)|ifnull(NULL, 3)|nullif(2, 2)|nullif(2, 3)|nvl(NULL, hello)|nvl2(first, second, third)|nvl2(NULL, second, third)|\n",
      "+-----------------------------+---------------+------------+------------+----------------+--------------------------+-------------------------+\n",
      "|                            4|              3|        NULL|           2|           hello|                    second|                    third|\n",
      "+-----------------------------+---------------+------------+------------+----------------+--------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select coalesce(null, null, 4, null),\n",
    "          ifnull(null, 3),\n",
    "          nullif(2, 2),\n",
    "          nullif(2 ,3),\n",
    "          nvl(null, \"hello\"),\n",
    "          nvl2(\"first\", \"second\", \"third\"),\n",
    "          nvl2(null, \"second\", \"third\")\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb36fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1968"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop\n",
    "df.na.drop(\"any\").count()   # if any value in row is null then the row is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5f49ec64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"all\").count()   # if all value of row is null then drop the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "62942eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"any\", subset=[\"StockCode\", \"InvoiceNo\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f5bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|All null Value|      56|2010-12-01 11:52:00|      0.0|      NULL|United Kingdom|\n",
      "|   536545|    21134|All null Value|       1|2010-12-01 14:32:00|      0.0|      NULL|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill\n",
    "df.na.fill(\"All null Value\").where(\"Description = 'All null Value'\").show(2)\n",
    "# fill(\"All null Value\") only applies to columns of type string. thats why CustomerID is still NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "93767e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|All null value|      56|2010-12-01 11:52:00|      0.0|       0.0|United Kingdom|\n",
      "|   536545|    21134|All null value|       1|2010-12-01 14:32:00|      0.0|       0.0|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill({\"Description\": \"All null value\", \"CustomerID\": 0}) \\\n",
    "    .where(\"Description = 'All null value'\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72150194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|   Description|\n",
      "+--------------+\n",
      "|All null value|\n",
      "|All null value|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    with fillnull as (\n",
    "          select ifnull(Description, \"All null value\") as Description\n",
    "    from retail_data\n",
    "    )\n",
    "    \n",
    "    select *\n",
    "    from fillnull\n",
    "          where Description='All null value'\n",
    "\"\"\") .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "437b3d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "df.na.replace(\"\", \"UNKNOWN\", \"Description\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d320e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null ordering\n",
    "from pyspark.sql.functions import desc, asc \n",
    "from pyspark.sql.functions import desc_nulls_first, desc_nulls_last, asc_nulls_first, asc_nulls_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "217e8820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|       NULL|      56|2010-12-01 11:52:00|      0.0|      NULL|United Kingdom|\n",
      "|   536550|    85044|       NULL|       1|2010-12-01 14:34:00|      0.0|      NULL|United Kingdom|\n",
      "|   536545|    21134|       NULL|       1|2010-12-01 14:32:00|      0.0|      NULL|United Kingdom|\n",
      "|   536546|    22145|       NULL|       1|2010-12-01 14:33:00|      0.0|      NULL|United Kingdom|\n",
      "|   536547|    37509|       NULL|       1|2010-12-01 14:33:00|      0.0|      NULL|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort((\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3e6744ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|       NULL|      56|2010-12-01 11:52:00|      0.0|      NULL|United Kingdom|\n",
      "|   536550|    85044|       NULL|       1|2010-12-01 14:34:00|      0.0|      NULL|United Kingdom|\n",
      "|   536545|    21134|       NULL|       1|2010-12-01 14:32:00|      0.0|      NULL|United Kingdom|\n",
      "|   536546|    22145|       NULL|       1|2010-12-01 14:33:00|      0.0|      NULL|United Kingdom|\n",
      "|   536547|    37509|       NULL|       1|2010-12-01 14:33:00|      0.0|      NULL|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(asc_nulls_first(\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "60754742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536592|    84832|ZINC WILLIE WINKI...|       1|2010-12-01 17:06:00|     1.66|      NULL|United Kingdom|\n",
      "|   536381|    84832|ZINC WILLIE WINKI...|       1|2010-12-01 09:41:00|     0.85|   15311.0|United Kingdom|\n",
      "|   536544|    84832|ZINC WILLIE WINKI...|       3|2010-12-01 14:32:00|     1.66|      NULL|United Kingdom|\n",
      "|   536522|    84836|ZINC METAL HEART ...|       1|2010-12-01 12:49:00|     1.25|   15012.0|United Kingdom|\n",
      "|   536446|    84836|ZINC METAL HEART ...|      12|2010-12-01 12:15:00|     1.25|   15983.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(desc(\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa61635",
   "metadata": {},
   "source": [
    "#### Complex types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "bb6e5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, array, create_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "6cd984ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|complex_type                                |\n",
      "+--------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER, 536365}|\n",
      "|{WHITE METAL LANTERN, 536365}               |\n",
      "+--------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#struct\n",
    "\n",
    "complex_df = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex_type\"))\n",
    "complex_df.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "9d414d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+\n",
      "|Description                       |InvoiceNo|\n",
      "+----------------------------------+---------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |\n",
      "|WHITE METAL LANTERN               |536365   |\n",
      "+----------------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_df.selectExpr(\"complex_type.Description\", \"complex_type.InvoiceNo\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1a7b6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array\n",
    "from pyspark.sql.functions import split, size, array_contains, explode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb7788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|split(Description,  , -1)               |\n",
      "+----------------------------------------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "|[WHITE, METAL, LANTERN]                 |\n",
      "+----------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "df.select(split(\"Description\", \" \")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d6cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------+\n",
      "|Description                       |lenght|\n",
      "+----------------------------------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|5     |\n",
      "|WHITE METAL LANTERN               |3     |\n",
      "+----------------------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# size\n",
    "df.select(\"Description\", size(split(\"Description\", \" \")).alias(\"lenght\")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01faa856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------------------------------------------+\n",
      "|split(Description,  , -1)               |array_contains(split(Description,  , -1), WHITE)|\n",
      "+----------------------------------------+------------------------------------------------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|true                                            |\n",
      "|[WHITE, METAL, LANTERN]                 |true                                            |\n",
      "+----------------------------------------+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#array_contains\n",
    "df.select(split(\"Description\", \" \"), array_contains(split(\"Description\", \" \"), \"WHITE\")).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "95995e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|CustomerID|    col|\n",
      "+----------+-------+\n",
      "|   17850.0|  WHITE|\n",
      "|   17850.0|HANGING|\n",
      "|   17850.0|  HEART|\n",
      "|   17850.0|T-LIGHT|\n",
      "|   17850.0| HOLDER|\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode\n",
    "df.select(\"CustomerID\", explode(split(\"Description\", \" \"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "3f2e7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "from pyspark.sql.functions import create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f7a62ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|complex_map                                  |\n",
      "+---------------------------------------------+\n",
      "|{17850 -> WHITE HANGING HEART T-LIGHT HOLDER}|\n",
      "|{17850 -> WHITE METAL LANTERN}               |\n",
      "+---------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_map = df.select(create_map(col(\"CustomerID\").cast(\"int\"), \"Description\").alias(\"complex_map\"))\n",
    "complex_map.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "247fbe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  complex_map[17850]|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_map.selectExpr(\"complex_map['17850']\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "370f5b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  complex_map[17850]|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "|CREAM CUPID HEART...|\n",
      "|KNITTED UNION FLA...|\n",
      "|RED WOOLLY HOTTIE...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_map.select(col(\"complex_map\").getItem(\"17850\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42afac8",
   "metadata": {},
   "source": [
    "#### User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "2354d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3674da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numdf = spark.range(5).toDF(\"num\")\n",
    "numdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "937bef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numdf.createOrReplaceTempView(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "d33d9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power3(num: int) -> int:\n",
    "    return num**3\n",
    "\n",
    "power3udf = udf(power3, IntegerType())\n",
    "# now one can power3udf detaframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "13fe1fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numdf.select(power3udf(\"num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "cfa8e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|POWER(num, 3)|\n",
      "+-------------+\n",
      "|          0.0|\n",
      "|          1.0|\n",
      "|          8.0|\n",
      "|         27.0|\n",
      "|         64.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numdf.select(power3(col(\"num\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "c3dbebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 09:35:14 WARN SimpleFunctionRegistry: The function power3 replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.power3(num: int) -> int>"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register power3 with spark to use as SQL\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "spark.udf.register(\"power3\", power3, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "fd96f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numdf.selectExpr(\"power3(num)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "dc8ff935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select power3(num)\n",
    "    from number\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "b6eb18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04088c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
