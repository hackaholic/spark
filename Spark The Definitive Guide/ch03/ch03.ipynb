{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d76fb1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e5b0056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ec1b131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 10:49:51 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ch03\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16c007b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kumar-rke2-1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ch03</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=ch03>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4617eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 305 items\n",
      "-rw-r--r--   2 hadoop supergroup     275001 2025-05-10 14:57 /data/retail-data/by-day/2010-12-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup     191826 2025-05-10 14:57 /data/retail-data/by-day/2010-12-02.csv\n",
      "-rw-r--r--   2 hadoop supergroup     190700 2025-05-10 14:57 /data/retail-data/by-day/2010-12-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup     246056 2025-05-10 14:57 /data/retail-data/by-day/2010-12-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup     339039 2025-05-10 14:57 /data/retail-data/by-day/2010-12-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     255832 2025-05-10 14:57 /data/retail-data/by-day/2010-12-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     235974 2025-05-10 14:57 /data/retail-data/by-day/2010-12-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup     252904 2025-05-10 14:58 /data/retail-data/by-day/2010-12-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup     241468 2025-05-10 14:57 /data/retail-data/by-day/2010-12-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup     132120 2025-05-10 14:57 /data/retail-data/by-day/2010-12-12.csv\n",
      "-rw-r--r--   2 hadoop supergroup     202021 2025-05-10 14:57 /data/retail-data/by-day/2010-12-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup     186685 2025-05-10 14:57 /data/retail-data/by-day/2010-12-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup     122496 2025-05-10 14:58 /data/retail-data/by-day/2010-12-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup     163452 2025-05-10 14:58 /data/retail-data/by-day/2010-12-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup     264944 2025-05-10 14:57 /data/retail-data/by-day/2010-12-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup      47264 2025-05-10 14:57 /data/retail-data/by-day/2010-12-19.csv\n",
      "-rw-r--r--   2 hadoop supergroup     153561 2025-05-10 14:57 /data/retail-data/by-day/2010-12-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     134904 2025-05-10 14:57 /data/retail-data/by-day/2010-12-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup      25836 2025-05-10 14:58 /data/retail-data/by-day/2010-12-22.csv\n",
      "-rw-r--r--   2 hadoop supergroup      83192 2025-05-10 14:57 /data/retail-data/by-day/2010-12-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup     104656 2025-05-10 14:57 /data/retail-data/by-day/2011-01-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup     153733 2025-05-10 14:57 /data/retail-data/by-day/2011-01-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup     162100 2025-05-10 14:57 /data/retail-data/by-day/2011-01-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     155938 2025-05-10 14:58 /data/retail-data/by-day/2011-01-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     101317 2025-05-10 14:57 /data/retail-data/by-day/2011-01-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup     170418 2025-05-10 14:58 /data/retail-data/by-day/2011-01-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup     127847 2025-05-10 14:57 /data/retail-data/by-day/2011-01-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup     158187 2025-05-10 14:57 /data/retail-data/by-day/2011-01-12.csv\n",
      "-rw-r--r--   2 hadoop supergroup     126109 2025-05-10 14:57 /data/retail-data/by-day/2011-01-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup     131086 2025-05-10 14:57 /data/retail-data/by-day/2011-01-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup      58906 2025-05-10 14:57 /data/retail-data/by-day/2011-01-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup     221821 2025-05-10 14:58 /data/retail-data/by-day/2011-01-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup     124711 2025-05-10 14:57 /data/retail-data/by-day/2011-01-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup     124865 2025-05-10 14:57 /data/retail-data/by-day/2011-01-19.csv\n",
      "-rw-r--r--   2 hadoop supergroup     130157 2025-05-10 14:57 /data/retail-data/by-day/2011-01-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     139157 2025-05-10 14:57 /data/retail-data/by-day/2011-01-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup      80012 2025-05-10 14:57 /data/retail-data/by-day/2011-01-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup     128813 2025-05-10 14:57 /data/retail-data/by-day/2011-01-24.csv\n",
      "-rw-r--r--   2 hadoop supergroup     146739 2025-05-10 14:57 /data/retail-data/by-day/2011-01-25.csv\n",
      "-rw-r--r--   2 hadoop supergroup     113985 2025-05-10 14:57 /data/retail-data/by-day/2011-01-26.csv\n",
      "-rw-r--r--   2 hadoop supergroup     140033 2025-05-10 14:57 /data/retail-data/by-day/2011-01-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup      87450 2025-05-10 14:57 /data/retail-data/by-day/2011-01-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup      66171 2025-05-10 14:57 /data/retail-data/by-day/2011-01-30.csv\n",
      "-rw-r--r--   2 hadoop supergroup     132774 2025-05-10 14:57 /data/retail-data/by-day/2011-01-31.csv\n",
      "-rw-r--r--   2 hadoop supergroup     139954 2025-05-10 14:58 /data/retail-data/by-day/2011-02-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup     124794 2025-05-10 14:57 /data/retail-data/by-day/2011-02-02.csv\n",
      "-rw-r--r--   2 hadoop supergroup      87779 2025-05-10 14:57 /data/retail-data/by-day/2011-02-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup     107014 2025-05-10 14:57 /data/retail-data/by-day/2011-02-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup      25516 2025-05-10 14:57 /data/retail-data/by-day/2011-02-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     111980 2025-05-10 14:57 /data/retail-data/by-day/2011-02-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     107136 2025-05-10 14:57 /data/retail-data/by-day/2011-02-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup      75999 2025-05-10 14:57 /data/retail-data/by-day/2011-02-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup      70942 2025-05-10 14:57 /data/retail-data/by-day/2011-02-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup      83807 2025-05-10 14:57 /data/retail-data/by-day/2011-02-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup      57098 2025-05-10 14:57 /data/retail-data/by-day/2011-02-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup      97341 2025-05-10 14:57 /data/retail-data/by-day/2011-02-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup     117745 2025-05-10 14:57 /data/retail-data/by-day/2011-02-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup     106100 2025-05-10 14:57 /data/retail-data/by-day/2011-02-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup     150882 2025-05-10 14:57 /data/retail-data/by-day/2011-02-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup      75633 2025-05-10 14:57 /data/retail-data/by-day/2011-02-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup      78068 2025-05-10 14:57 /data/retail-data/by-day/2011-02-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     124088 2025-05-10 14:58 /data/retail-data/by-day/2011-02-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup     142089 2025-05-10 14:57 /data/retail-data/by-day/2011-02-22.csv\n",
      "-rw-r--r--   2 hadoop supergroup     138337 2025-05-10 14:57 /data/retail-data/by-day/2011-02-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup     116769 2025-05-10 14:57 /data/retail-data/by-day/2011-02-24.csv\n",
      "-rw-r--r--   2 hadoop supergroup      87507 2025-05-10 14:57 /data/retail-data/by-day/2011-02-25.csv\n",
      "-rw-r--r--   2 hadoop supergroup      74111 2025-05-10 14:57 /data/retail-data/by-day/2011-02-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup     151947 2025-05-10 14:57 /data/retail-data/by-day/2011-02-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup     119793 2025-05-10 14:57 /data/retail-data/by-day/2011-03-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup      85302 2025-05-10 14:57 /data/retail-data/by-day/2011-03-02.csv\n",
      "-rw-r--r--   2 hadoop supergroup     122010 2025-05-10 14:57 /data/retail-data/by-day/2011-03-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup      97942 2025-05-10 14:57 /data/retail-data/by-day/2011-03-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup      76977 2025-05-10 14:58 /data/retail-data/by-day/2011-03-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     172784 2025-05-10 14:58 /data/retail-data/by-day/2011-03-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     141880 2025-05-10 14:57 /data/retail-data/by-day/2011-03-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup     113715 2025-05-10 14:57 /data/retail-data/by-day/2011-03-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup     100881 2025-05-10 14:58 /data/retail-data/by-day/2011-03-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup      84393 2025-05-10 14:57 /data/retail-data/by-day/2011-03-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup      48917 2025-05-10 14:57 /data/retail-data/by-day/2011-03-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup      99595 2025-05-10 14:57 /data/retail-data/by-day/2011-03-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup     112853 2025-05-10 14:57 /data/retail-data/by-day/2011-03-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup      78923 2025-05-10 14:57 /data/retail-data/by-day/2011-03-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup     180245 2025-05-10 14:57 /data/retail-data/by-day/2011-03-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup     122082 2025-05-10 14:57 /data/retail-data/by-day/2011-03-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup     130096 2025-05-10 14:57 /data/retail-data/by-day/2011-03-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup      96588 2025-05-10 14:58 /data/retail-data/by-day/2011-03-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup     206516 2025-05-10 14:57 /data/retail-data/by-day/2011-03-22.csv\n",
      "-rw-r--r--   2 hadoop supergroup     116861 2025-05-10 14:57 /data/retail-data/by-day/2011-03-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup     141988 2025-05-10 14:57 /data/retail-data/by-day/2011-03-24.csv\n",
      "-rw-r--r--   2 hadoop supergroup     122156 2025-05-10 14:57 /data/retail-data/by-day/2011-03-25.csv\n",
      "-rw-r--r--   2 hadoop supergroup      67786 2025-05-10 14:57 /data/retail-data/by-day/2011-03-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup     147212 2025-05-10 14:57 /data/retail-data/by-day/2011-03-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup     173104 2025-05-10 14:57 /data/retail-data/by-day/2011-03-29.csv\n",
      "-rw-r--r--   2 hadoop supergroup     141435 2025-05-10 14:57 /data/retail-data/by-day/2011-03-30.csv\n",
      "-rw-r--r--   2 hadoop supergroup     152420 2025-05-10 14:58 /data/retail-data/by-day/2011-03-31.csv\n",
      "-rw-r--r--   2 hadoop supergroup     112493 2025-05-10 14:57 /data/retail-data/by-day/2011-04-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup      66518 2025-05-10 14:57 /data/retail-data/by-day/2011-04-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup     150711 2025-05-10 14:58 /data/retail-data/by-day/2011-04-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup     107560 2025-05-10 14:57 /data/retail-data/by-day/2011-04-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup      98068 2025-05-10 14:57 /data/retail-data/by-day/2011-04-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     136252 2025-05-10 14:57 /data/retail-data/by-day/2011-04-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     125969 2025-05-10 14:57 /data/retail-data/by-day/2011-04-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup      85637 2025-05-10 14:57 /data/retail-data/by-day/2011-04-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup     112516 2025-05-10 14:57 /data/retail-data/by-day/2011-04-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup     104116 2025-05-10 14:57 /data/retail-data/by-day/2011-04-12.csv\n",
      "-rw-r--r--   2 hadoop supergroup      95007 2025-05-10 14:57 /data/retail-data/by-day/2011-04-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup     144236 2025-05-10 14:57 /data/retail-data/by-day/2011-04-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup     128980 2025-05-10 14:57 /data/retail-data/by-day/2011-04-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup      87658 2025-05-10 14:57 /data/retail-data/by-day/2011-04-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup     280118 2025-05-10 14:57 /data/retail-data/by-day/2011-04-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup     165965 2025-05-10 14:58 /data/retail-data/by-day/2011-04-19.csv\n",
      "-rw-r--r--   2 hadoop supergroup      91865 2025-05-10 14:57 /data/retail-data/by-day/2011-04-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     170023 2025-05-10 14:57 /data/retail-data/by-day/2011-04-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup     167480 2025-05-10 14:57 /data/retail-data/by-day/2011-04-26.csv\n",
      "-rw-r--r--   2 hadoop supergroup     116613 2025-05-10 14:57 /data/retail-data/by-day/2011-04-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup     106481 2025-05-10 14:57 /data/retail-data/by-day/2011-04-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup      40884 2025-05-10 14:58 /data/retail-data/by-day/2011-05-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup     126563 2025-05-10 14:57 /data/retail-data/by-day/2011-05-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup     105162 2025-05-10 14:57 /data/retail-data/by-day/2011-05-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup     162318 2025-05-10 14:57 /data/retail-data/by-day/2011-05-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup     180297 2025-05-10 14:57 /data/retail-data/by-day/2011-05-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     136006 2025-05-10 14:57 /data/retail-data/by-day/2011-05-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup     154043 2025-05-10 14:57 /data/retail-data/by-day/2011-05-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup     224383 2025-05-10 14:58 /data/retail-data/by-day/2011-05-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup     151121 2025-05-10 14:57 /data/retail-data/by-day/2011-05-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup     175096 2025-05-10 14:57 /data/retail-data/by-day/2011-05-12.csv\n",
      "-rw-r--r--   2 hadoop supergroup     125524 2025-05-10 14:57 /data/retail-data/by-day/2011-05-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup      73038 2025-05-10 14:57 /data/retail-data/by-day/2011-05-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup     140801 2025-05-10 14:57 /data/retail-data/by-day/2011-05-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup     168847 2025-05-10 14:57 /data/retail-data/by-day/2011-05-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup     121774 2025-05-10 14:57 /data/retail-data/by-day/2011-05-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup     172737 2025-05-10 14:57 /data/retail-data/by-day/2011-05-19.csv\n",
      "-rw-r--r--   2 hadoop supergroup     100198 2025-05-10 14:57 /data/retail-data/by-day/2011-05-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     141114 2025-05-10 14:57 /data/retail-data/by-day/2011-05-22.csv\n",
      "-rw-r--r--   2 hadoop supergroup     144869 2025-05-10 14:57 /data/retail-data/by-day/2011-05-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup     153615 2025-05-10 14:57 /data/retail-data/by-day/2011-05-24.csv\n",
      "-rw-r--r--   2 hadoop supergroup     116151 2025-05-10 14:57 /data/retail-data/by-day/2011-05-25.csv\n",
      "-rw-r--r--   2 hadoop supergroup      86137 2025-05-10 14:57 /data/retail-data/by-day/2011-05-26.csv\n",
      "-rw-r--r--   2 hadoop supergroup     103064 2025-05-10 14:57 /data/retail-data/by-day/2011-05-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup      57243 2025-05-10 14:57 /data/retail-data/by-day/2011-05-29.csv\n",
      "-rw-r--r--   2 hadoop supergroup     112113 2025-05-10 14:57 /data/retail-data/by-day/2011-05-31.csv\n",
      "-rw-r--r--   2 hadoop supergroup     126676 2025-05-10 14:57 /data/retail-data/by-day/2011-06-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup     139233 2025-05-10 14:57 /data/retail-data/by-day/2011-06-02.csv\n",
      "-rw-r--r--   2 hadoop supergroup      75454 2025-05-10 14:57 /data/retail-data/by-day/2011-06-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup     140061 2025-05-10 14:57 /data/retail-data/by-day/2011-06-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup     109829 2025-05-10 14:57 /data/retail-data/by-day/2011-06-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     171241 2025-05-10 14:57 /data/retail-data/by-day/2011-06-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     203086 2025-05-10 14:57 /data/retail-data/by-day/2011-06-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup     162227 2025-05-10 14:57 /data/retail-data/by-day/2011-06-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup      93701 2025-05-10 14:57 /data/retail-data/by-day/2011-06-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup      97835 2025-05-10 14:57 /data/retail-data/by-day/2011-06-12.csv\n",
      "-rw-r--r--   2 hadoop supergroup     129767 2025-05-10 14:58 /data/retail-data/by-day/2011-06-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup     140214 2025-05-10 14:57 /data/retail-data/by-day/2011-06-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup     151127 2025-05-10 14:57 /data/retail-data/by-day/2011-06-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup     136339 2025-05-10 14:57 /data/retail-data/by-day/2011-06-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup      85739 2025-05-10 14:57 /data/retail-data/by-day/2011-06-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup     105190 2025-05-10 14:58 /data/retail-data/by-day/2011-06-19.csv\n",
      "-rw-r--r--   2 hadoop supergroup     175539 2025-05-10 14:58 /data/retail-data/by-day/2011-06-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     134322 2025-05-10 14:57 /data/retail-data/by-day/2011-06-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup      91369 2025-05-10 14:58 /data/retail-data/by-day/2011-06-22.csv\n",
      "-rw-r--r--   2 hadoop supergroup     175463 2025-05-10 14:57 /data/retail-data/by-day/2011-06-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup      91308 2025-05-10 14:57 /data/retail-data/by-day/2011-06-24.csv\n",
      "-rw-r--r--   2 hadoop supergroup      64579 2025-05-10 14:58 /data/retail-data/by-day/2011-06-26.csv\n",
      "-rw-r--r--   2 hadoop supergroup     104370 2025-05-10 14:57 /data/retail-data/by-day/2011-06-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup      92153 2025-05-10 14:57 /data/retail-data/by-day/2011-06-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup     117488 2025-05-10 14:57 /data/retail-data/by-day/2011-06-29.csv\n",
      "-rw-r--r--   2 hadoop supergroup     143253 2025-05-10 14:57 /data/retail-data/by-day/2011-06-30.csv\n",
      "-rw-r--r--   2 hadoop supergroup      90752 2025-05-10 14:57 /data/retail-data/by-day/2011-07-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup      54798 2025-05-10 14:58 /data/retail-data/by-day/2011-07-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup     189204 2025-05-10 14:57 /data/retail-data/by-day/2011-07-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup     189831 2025-05-10 14:57 /data/retail-data/by-day/2011-07-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup     159436 2025-05-10 14:57 /data/retail-data/by-day/2011-07-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     171634 2025-05-10 14:58 /data/retail-data/by-day/2011-07-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     140138 2025-05-10 14:57 /data/retail-data/by-day/2011-07-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup      74191 2025-05-10 14:57 /data/retail-data/by-day/2011-07-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup     133475 2025-05-10 14:57 /data/retail-data/by-day/2011-07-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup     144196 2025-05-10 14:57 /data/retail-data/by-day/2011-07-12.csv\n",
      "-rw-r--r--   2 hadoop supergroup     144027 2025-05-10 14:57 /data/retail-data/by-day/2011-07-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup     153670 2025-05-10 14:57 /data/retail-data/by-day/2011-07-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup      99926 2025-05-10 14:57 /data/retail-data/by-day/2011-07-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup     112983 2025-05-10 14:57 /data/retail-data/by-day/2011-07-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup     194171 2025-05-10 14:57 /data/retail-data/by-day/2011-07-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup     147143 2025-05-10 14:57 /data/retail-data/by-day/2011-07-19.csv\n",
      "-rw-r--r--   2 hadoop supergroup     177416 2025-05-10 14:57 /data/retail-data/by-day/2011-07-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     140592 2025-05-10 14:57 /data/retail-data/by-day/2011-07-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup     113759 2025-05-10 14:57 /data/retail-data/by-day/2011-07-22.csv\n",
      "-rw-r--r--   2 hadoop supergroup     101031 2025-05-10 14:58 /data/retail-data/by-day/2011-07-24.csv\n",
      "-rw-r--r--   2 hadoop supergroup     170093 2025-05-10 14:57 /data/retail-data/by-day/2011-07-25.csv\n",
      "-rw-r--r--   2 hadoop supergroup     112780 2025-05-10 14:57 /data/retail-data/by-day/2011-07-26.csv\n",
      "-rw-r--r--   2 hadoop supergroup     109514 2025-05-10 14:57 /data/retail-data/by-day/2011-07-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup     138826 2025-05-10 14:57 /data/retail-data/by-day/2011-07-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup     101094 2025-05-10 14:57 /data/retail-data/by-day/2011-07-29.csv\n",
      "-rw-r--r--   2 hadoop supergroup     114650 2025-05-10 14:57 /data/retail-data/by-day/2011-07-31.csv\n",
      "-rw-r--r--   2 hadoop supergroup     107491 2025-05-10 14:57 /data/retail-data/by-day/2011-08-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup     120138 2025-05-10 14:57 /data/retail-data/by-day/2011-08-02.csv\n",
      "-rw-r--r--   2 hadoop supergroup     140576 2025-05-10 14:57 /data/retail-data/by-day/2011-08-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup     162548 2025-05-10 14:57 /data/retail-data/by-day/2011-08-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup     127923 2025-05-10 14:57 /data/retail-data/by-day/2011-08-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup      49245 2025-05-10 14:57 /data/retail-data/by-day/2011-08-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     131650 2025-05-10 14:57 /data/retail-data/by-day/2011-08-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup     100002 2025-05-10 14:57 /data/retail-data/by-day/2011-08-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup     120046 2025-05-10 14:57 /data/retail-data/by-day/2011-08-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup     175951 2025-05-10 14:57 /data/retail-data/by-day/2011-08-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup      97516 2025-05-10 14:57 /data/retail-data/by-day/2011-08-12.csv\n",
      "-rw-r--r--   2 hadoop supergroup      50684 2025-05-10 14:57 /data/retail-data/by-day/2011-08-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup      82959 2025-05-10 14:57 /data/retail-data/by-day/2011-08-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup      92486 2025-05-10 14:57 /data/retail-data/by-day/2011-08-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup     148605 2025-05-10 14:57 /data/retail-data/by-day/2011-08-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup     137689 2025-05-10 14:57 /data/retail-data/by-day/2011-08-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup      72940 2025-05-10 14:57 /data/retail-data/by-day/2011-08-19.csv\n",
      "-rw-r--r--   2 hadoop supergroup      97031 2025-05-10 14:58 /data/retail-data/by-day/2011-08-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup     114812 2025-05-10 14:57 /data/retail-data/by-day/2011-08-22.csv\n",
      "-rw-r--r--   2 hadoop supergroup     130326 2025-05-10 14:57 /data/retail-data/by-day/2011-08-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup     167436 2025-05-10 14:57 /data/retail-data/by-day/2011-08-24.csv\n",
      "-rw-r--r--   2 hadoop supergroup     118982 2025-05-10 14:57 /data/retail-data/by-day/2011-08-25.csv\n",
      "-rw-r--r--   2 hadoop supergroup      82531 2025-05-10 14:58 /data/retail-data/by-day/2011-08-26.csv\n",
      "-rw-r--r--   2 hadoop supergroup     107760 2025-05-10 14:57 /data/retail-data/by-day/2011-08-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup     274692 2025-05-10 14:57 /data/retail-data/by-day/2011-08-30.csv\n",
      "-rw-r--r--   2 hadoop supergroup     113373 2025-05-10 14:57 /data/retail-data/by-day/2011-08-31.csv\n",
      "-rw-r--r--   2 hadoop supergroup     127664 2025-05-10 14:57 /data/retail-data/by-day/2011-09-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup     205036 2025-05-10 14:57 /data/retail-data/by-day/2011-09-02.csv\n",
      "-rw-r--r--   2 hadoop supergroup     120645 2025-05-10 14:57 /data/retail-data/by-day/2011-09-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup     141625 2025-05-10 14:57 /data/retail-data/by-day/2011-09-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup     102669 2025-05-10 14:57 /data/retail-data/by-day/2011-09-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     181656 2025-05-10 14:57 /data/retail-data/by-day/2011-09-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     156826 2025-05-10 14:57 /data/retail-data/by-day/2011-09-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup     142210 2025-05-10 14:57 /data/retail-data/by-day/2011-09-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup     185196 2025-05-10 14:57 /data/retail-data/by-day/2011-09-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup     154124 2025-05-10 14:57 /data/retail-data/by-day/2011-09-12.csv\n",
      "-rw-r--r--   2 hadoop supergroup     217571 2025-05-10 14:57 /data/retail-data/by-day/2011-09-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup     123164 2025-05-10 14:57 /data/retail-data/by-day/2011-09-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup     195419 2025-05-10 14:57 /data/retail-data/by-day/2011-09-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup     131715 2025-05-10 14:57 /data/retail-data/by-day/2011-09-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup     116452 2025-05-10 14:57 /data/retail-data/by-day/2011-09-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup     128010 2025-05-10 14:57 /data/retail-data/by-day/2011-09-19.csv\n",
      "-rw-r--r--   2 hadoop supergroup     153186 2025-05-10 14:57 /data/retail-data/by-day/2011-09-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     272947 2025-05-10 14:57 /data/retail-data/by-day/2011-09-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup     246699 2025-05-10 14:58 /data/retail-data/by-day/2011-09-22.csv\n",
      "-rw-r--r--   2 hadoop supergroup     233544 2025-05-10 14:57 /data/retail-data/by-day/2011-09-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup     182072 2025-05-10 14:57 /data/retail-data/by-day/2011-09-25.csv\n",
      "-rw-r--r--   2 hadoop supergroup     141083 2025-05-10 14:57 /data/retail-data/by-day/2011-09-26.csv\n",
      "-rw-r--r--   2 hadoop supergroup     168998 2025-05-10 14:58 /data/retail-data/by-day/2011-09-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup     255103 2025-05-10 14:57 /data/retail-data/by-day/2011-09-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup     225953 2025-05-10 14:57 /data/retail-data/by-day/2011-09-29.csv\n",
      "-rw-r--r--   2 hadoop supergroup     168344 2025-05-10 14:57 /data/retail-data/by-day/2011-09-30.csv\n",
      "-rw-r--r--   2 hadoop supergroup     130864 2025-05-10 14:57 /data/retail-data/by-day/2011-10-02.csv\n",
      "-rw-r--r--   2 hadoop supergroup     205031 2025-05-10 14:58 /data/retail-data/by-day/2011-10-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup     249521 2025-05-10 14:57 /data/retail-data/by-day/2011-10-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup     219340 2025-05-10 14:57 /data/retail-data/by-day/2011-10-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup     285739 2025-05-10 14:57 /data/retail-data/by-day/2011-10-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     216893 2025-05-10 14:57 /data/retail-data/by-day/2011-10-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     118425 2025-05-10 14:57 /data/retail-data/by-day/2011-10-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup     289700 2025-05-10 14:57 /data/retail-data/by-day/2011-10-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup     217528 2025-05-10 14:57 /data/retail-data/by-day/2011-10-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup     192765 2025-05-10 14:57 /data/retail-data/by-day/2011-10-12.csv\n",
      "-rw-r--r--   2 hadoop supergroup     209708 2025-05-10 14:57 /data/retail-data/by-day/2011-10-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup     162184 2025-05-10 14:58 /data/retail-data/by-day/2011-10-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup     114470 2025-05-10 14:57 /data/retail-data/by-day/2011-10-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup     261910 2025-05-10 14:57 /data/retail-data/by-day/2011-10-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup     257244 2025-05-10 14:58 /data/retail-data/by-day/2011-10-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup     206689 2025-05-10 14:57 /data/retail-data/by-day/2011-10-19.csv\n",
      "-rw-r--r--   2 hadoop supergroup     212077 2025-05-10 14:57 /data/retail-data/by-day/2011-10-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     127547 2025-05-10 14:58 /data/retail-data/by-day/2011-10-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup     158316 2025-05-10 14:57 /data/retail-data/by-day/2011-10-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup     263839 2025-05-10 14:57 /data/retail-data/by-day/2011-10-24.csv\n",
      "-rw-r--r--   2 hadoop supergroup     223947 2025-05-10 14:58 /data/retail-data/by-day/2011-10-25.csv\n",
      "-rw-r--r--   2 hadoop supergroup     174790 2025-05-10 14:57 /data/retail-data/by-day/2011-10-26.csv\n",
      "-rw-r--r--   2 hadoop supergroup     228883 2025-05-10 14:57 /data/retail-data/by-day/2011-10-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup     146761 2025-05-10 14:57 /data/retail-data/by-day/2011-10-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup     264885 2025-05-10 14:57 /data/retail-data/by-day/2011-10-30.csv\n",
      "-rw-r--r--   2 hadoop supergroup     298653 2025-05-10 14:57 /data/retail-data/by-day/2011-10-31.csv\n",
      "-rw-r--r--   2 hadoop supergroup     161872 2025-05-10 14:57 /data/retail-data/by-day/2011-11-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup     233949 2025-05-10 14:57 /data/retail-data/by-day/2011-11-02.csv\n",
      "-rw-r--r--   2 hadoop supergroup     216844 2025-05-10 14:57 /data/retail-data/by-day/2011-11-03.csv\n",
      "-rw-r--r--   2 hadoop supergroup     270087 2025-05-10 14:57 /data/retail-data/by-day/2011-11-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup     313153 2025-05-10 14:57 /data/retail-data/by-day/2011-11-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     188148 2025-05-10 14:57 /data/retail-data/by-day/2011-11-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     359329 2025-05-10 14:57 /data/retail-data/by-day/2011-11-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup     244073 2025-05-10 14:58 /data/retail-data/by-day/2011-11-09.csv\n",
      "-rw-r--r--   2 hadoop supergroup     295607 2025-05-10 14:57 /data/retail-data/by-day/2011-11-10.csv\n",
      "-rw-r--r--   2 hadoop supergroup     359019 2025-05-10 14:57 /data/retail-data/by-day/2011-11-11.csv\n",
      "-rw-r--r--   2 hadoop supergroup     272682 2025-05-10 14:57 /data/retail-data/by-day/2011-11-13.csv\n",
      "-rw-r--r--   2 hadoop supergroup     322946 2025-05-10 14:57 /data/retail-data/by-day/2011-11-14.csv\n",
      "-rw-r--r--   2 hadoop supergroup     304442 2025-05-10 14:57 /data/retail-data/by-day/2011-11-15.csv\n",
      "-rw-r--r--   2 hadoop supergroup     371320 2025-05-10 14:57 /data/retail-data/by-day/2011-11-16.csv\n",
      "-rw-r--r--   2 hadoop supergroup     321770 2025-05-10 14:57 /data/retail-data/by-day/2011-11-17.csv\n",
      "-rw-r--r--   2 hadoop supergroup     258456 2025-05-10 14:57 /data/retail-data/by-day/2011-11-18.csv\n",
      "-rw-r--r--   2 hadoop supergroup     302441 2025-05-10 14:57 /data/retail-data/by-day/2011-11-20.csv\n",
      "-rw-r--r--   2 hadoop supergroup     262095 2025-05-10 14:57 /data/retail-data/by-day/2011-11-21.csv\n",
      "-rw-r--r--   2 hadoop supergroup     351999 2025-05-10 14:57 /data/retail-data/by-day/2011-11-22.csv\n",
      "-rw-r--r--   2 hadoop supergroup     327359 2025-05-10 14:58 /data/retail-data/by-day/2011-11-23.csv\n",
      "-rw-r--r--   2 hadoop supergroup     331824 2025-05-10 14:58 /data/retail-data/by-day/2011-11-24.csv\n",
      "-rw-r--r--   2 hadoop supergroup     273837 2025-05-10 14:57 /data/retail-data/by-day/2011-11-25.csv\n",
      "-rw-r--r--   2 hadoop supergroup     230974 2025-05-10 14:57 /data/retail-data/by-day/2011-11-27.csv\n",
      "-rw-r--r--   2 hadoop supergroup     297785 2025-05-10 14:57 /data/retail-data/by-day/2011-11-28.csv\n",
      "-rw-r--r--   2 hadoop supergroup     381332 2025-05-10 14:57 /data/retail-data/by-day/2011-11-29.csv\n",
      "-rw-r--r--   2 hadoop supergroup     303485 2025-05-10 14:57 /data/retail-data/by-day/2011-11-30.csv\n",
      "-rw-r--r--   2 hadoop supergroup     259173 2025-05-10 14:57 /data/retail-data/by-day/2011-12-01.csv\n",
      "-rw-r--r--   2 hadoop supergroup     254858 2025-05-10 14:58 /data/retail-data/by-day/2011-12-02.csv\n",
      "-rw-r--r--   2 hadoop supergroup     184959 2025-05-10 14:57 /data/retail-data/by-day/2011-12-04.csv\n",
      "-rw-r--r--   2 hadoop supergroup     472154 2025-05-10 14:57 /data/retail-data/by-day/2011-12-05.csv\n",
      "-rw-r--r--   2 hadoop supergroup     300750 2025-05-10 14:57 /data/retail-data/by-day/2011-12-06.csv\n",
      "-rw-r--r--   2 hadoop supergroup     217714 2025-05-10 14:57 /data/retail-data/by-day/2011-12-07.csv\n",
      "-rw-r--r--   2 hadoop supergroup     432962 2025-05-10 14:57 /data/retail-data/by-day/2011-12-08.csv\n",
      "-rw-r--r--   2 hadoop supergroup     140637 2025-05-10 14:58 /data/retail-data/by-day/2011-12-09.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /data/retail-data/by-day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d89d6679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static_data_frame = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"hdfs:///data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7b70191a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_data_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "69f4f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data_frame.createOrReplaceTempView(\"retail_data\")\n",
    "static_schema = static_data_frame.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d41f7",
   "metadata": {},
   "source": [
    "Which customer making larger purchase day by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "20dd22cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n",
      "|InvoiceDate|CustomerID|total_cost|\n",
      "+-----------+----------+----------+\n",
      "| 2011-09-20|   17450.0|  71601.44|\n",
      "| 2011-09-15|   18102.0|  31661.54|\n",
      "| 2011-10-21|   18102.0|  29693.82|\n",
      "| 2010-12-07|   18102.0|  25920.37|\n",
      "| 2011-10-20|   14646.0|  25833.56|\n",
      "+-----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# SQL way\n",
    "spark.sql(\"\"\"\n",
    "    select DATE(InvoiceDate), CustomerID, round(sum(UnitPrice * Quantity), 2) as total_cost\n",
    "    from retail_data\n",
    "    where CustomerID is not NULL\n",
    "    group by CustomerID, DATE(InvoiceDate)\n",
    "    order by total_cost desc\n",
    "    limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "12b90bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc, window, to_date, sum as _sum, round\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51262ae8",
   "metadata": {},
   "source": [
    "#### window\n",
    "\n",
    "<pre>\n",
    "window() is used to group data into time-based buckets for aggregations on streaming or static datasets with timestamp/date columns.\n",
    "For \"1 day\" window, record with timestamp 2025-05-07 12:04:56 falls into:\n",
    "Window: \n",
    "start = 2025-05-07 00:00:00\n",
    "end =   2025-05-08 00:00:00\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "24f8c9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+-------------------+\n",
      "|CustomerID|              window|    cost|        InvoiceDate|\n",
      "+----------+--------------------+--------+-------------------+\n",
      "|   17450.0|{2011-09-20 00:00...|71601.44|2011-09-20 00:00:00|\n",
      "|   18102.0|{2011-09-15 00:00...|31661.54|2011-09-15 00:00:00|\n",
      "|   18102.0|{2011-10-21 00:00...|29693.82|2011-10-21 00:00:00|\n",
      "|   18102.0|{2010-12-07 00:00...|25920.37|2010-12-07 00:00:00|\n",
      "|   14646.0|{2011-10-20 00:00...|25833.56|2011-10-20 00:00:00|\n",
      "+----------+--------------------+--------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static_data_frame.selectExpr(\n",
    "    \"InvoiceDate\",\n",
    "    \"CustomerID\", \n",
    "    \"(UnitPrice * Quantity) as cost\") \\\n",
    "    .where(col(\"CustomerID\").isNotNull()) \\\n",
    "    .groupBy(col(\"CustomerID\"), window(col(\"InvoiceDate\"), \"1 day\")) \\\n",
    "    .agg(round(_sum(col(\"cost\")), 2).alias(\"cost\")) \\\n",
    "    .withColumn(\"InvoiceDate\", col(\"window\").getItem(\"start\")) \\\n",
    "    .orderBy(desc(\"cost\")) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43b2c2",
   "metadata": {},
   "source": [
    "#### readStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0dbf4d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "               .schema(static_schema) \\\n",
    "               .option(\"maxFilesPerTrigger\", 100) \\\n",
    "               .format(\"csv\") \\\n",
    "               .option(\"header\", True) \\\n",
    "               .load(\"hdfs:///data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f0e683cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_day = streaming_df \\\n",
    "            .selectExpr(\n",
    "                \"InvoiceDate\",\n",
    "                \"CustomerID\", \n",
    "                \"(UnitPrice * Quantity) as cost\") \\\n",
    "            .where(col(\"CustomerID\").isNotNull()) \\\n",
    "            .groupBy(\"CustomerID\", window(col(\"InvoiceDate\"), \"1 day\")) \\\n",
    "            .agg(round(_sum(col(\"cost\")), 2).alias(\"total_cost\")) \\\n",
    "            .orderBy(desc(\"total_cost\"))         \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b6f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 09:43:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-66b528eb-6587-4bb7-9189-11abf24a966e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/11 09:43:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f1556bc1de0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 09:43:50 WARN FileStreamSource: Listed 305 file(s) in 2765 ms          \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+--------------------+----------+\n",
      "|CustomerID|              window|total_cost|\n",
      "+----------+--------------------+----------+\n",
      "|   12931.0|{2011-08-04 00:00...|  19045.48|\n",
      "|   17949.0|{2011-06-30 00:00...|  18854.78|\n",
      "|   14646.0|{2011-03-29 00:00...|   18247.5|\n",
      "|   16684.0|{2011-10-05 00:00...|  18047.42|\n",
      "|   14156.0|{2011-01-14 00:00...|  16774.72|\n",
      "|   12415.0|{2011-03-03 00:00...|  16558.14|\n",
      "|   12415.0|{2011-10-05 00:00...|  16471.77|\n",
      "|   18102.0|{2011-06-14 00:00...|  14471.92|\n",
      "|   12415.0|{2011-02-15 00:00...|  14022.92|\n",
      "|   12415.0|{2011-05-17 00:00...|   11924.8|\n",
      "|   18102.0|{2011-02-07 00:00...|  10535.48|\n",
      "|   14646.0|{2011-01-14 00:00...|  10389.06|\n",
      "|   15769.0|{2011-03-17 00:00...|   10065.0|\n",
      "|   14646.0|{2011-11-10 00:00...|   9823.12|\n",
      "|   18102.0|{2011-05-17 00:00...|   9270.08|\n",
      "|   18102.0|{2011-05-16 00:00...|   8895.66|\n",
      "|   14156.0|{2011-09-05 00:00...|   8343.86|\n",
      "|   14646.0|{2011-09-13 00:00...|   8281.12|\n",
      "|   14646.0|{2011-01-21 00:00...|    8060.3|\n",
      "|   14088.0|{2011-06-02 00:00...|   7809.93|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------+--------------------+----------+\n",
      "|CustomerID|              window|total_cost|\n",
      "+----------+--------------------+----------+\n",
      "|   17450.0|{2011-09-20 00:00...|  71601.44|\n",
      "|   18102.0|{2011-09-15 00:00...|  31661.54|\n",
      "|   18102.0|{2010-12-07 00:00...|  25920.37|\n",
      "|   14646.0|{2011-10-20 00:00...|  25833.56|\n",
      "|   12415.0|{2011-06-15 00:00...|  23426.81|\n",
      "|   12415.0|{2011-08-18 00:00...|  21880.44|\n",
      "|   14646.0|{2011-08-11 00:00...|  19150.66|\n",
      "|   12931.0|{2011-08-04 00:00...|  19045.48|\n",
      "|   17949.0|{2011-06-30 00:00...|  18854.78|\n",
      "|   14646.0|{2011-03-29 00:00...|   18247.5|\n",
      "|   16684.0|{2011-10-05 00:00...|  18047.42|\n",
      "|   14156.0|{2011-01-14 00:00...|  16774.72|\n",
      "|   12415.0|{2011-03-03 00:00...|  16558.14|\n",
      "|   14646.0|{2011-05-12 00:00...|  16478.46|\n",
      "|   12415.0|{2011-10-05 00:00...|  16471.77|\n",
      "|   17450.0|{2011-08-17 00:00...|   16084.9|\n",
      "|   14646.0|{2011-09-19 00:00...|  15618.28|\n",
      "|   18102.0|{2011-11-04 00:00...|   15045.0|\n",
      "|   18102.0|{2011-06-14 00:00...|  14471.92|\n",
      "|   12415.0|{2011-02-15 00:00...|  14022.92|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----------+--------------------+----------+\n",
      "|CustomerID|              window|total_cost|\n",
      "+----------+--------------------+----------+\n",
      "|   17450.0|{2011-09-20 00:00...|  71601.44|\n",
      "|   18102.0|{2011-09-15 00:00...|  31661.54|\n",
      "|   18102.0|{2011-10-21 00:00...|  29693.82|\n",
      "|   18102.0|{2010-12-07 00:00...|  25920.37|\n",
      "|   14646.0|{2011-10-20 00:00...|  25833.56|\n",
      "|   12415.0|{2011-06-15 00:00...|  23426.81|\n",
      "|   15749.0|{2011-01-11 00:00...|   22998.4|\n",
      "|   18102.0|{2011-10-03 00:00...|  22429.69|\n",
      "|   12415.0|{2011-08-18 00:00...|  21880.44|\n",
      "|   14646.0|{2011-08-11 00:00...|  19150.66|\n",
      "|   12931.0|{2011-08-04 00:00...|  19045.48|\n",
      "|   17949.0|{2011-06-30 00:00...|  18854.78|\n",
      "|   17450.0|{2011-01-11 00:00...|   18620.2|\n",
      "|   14646.0|{2011-03-29 00:00...|   18247.5|\n",
      "|   16684.0|{2011-10-05 00:00...|  18047.42|\n",
      "|   14156.0|{2011-01-14 00:00...|  16774.72|\n",
      "|   12415.0|{2011-03-03 00:00...|  16558.14|\n",
      "|   18102.0|{2011-06-09 00:00...|   16488.0|\n",
      "|   14646.0|{2011-05-12 00:00...|  16478.46|\n",
      "|   12415.0|{2011-10-05 00:00...|  16471.77|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 09:45:04 WARN FileStreamSource: Listed 305 file(s) in 2569 ms          \n",
      "[Stage 129:===================================================> (195 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|CustomerID|              window|total_cost|\n",
      "+----------+--------------------+----------+\n",
      "|   17450.0|{2011-09-20 00:00...|  71601.44|\n",
      "|   18102.0|{2011-09-15 00:00...|  31661.54|\n",
      "|   18102.0|{2011-10-21 00:00...|  29693.82|\n",
      "|   18102.0|{2010-12-07 00:00...|  25920.37|\n",
      "|   14646.0|{2011-10-20 00:00...|  25833.56|\n",
      "|   12415.0|{2011-06-15 00:00...|  23426.81|\n",
      "|   15749.0|{2011-01-11 00:00...|   22998.4|\n",
      "|   18102.0|{2011-10-03 00:00...|  22429.69|\n",
      "|   12415.0|{2011-08-18 00:00...|  21880.44|\n",
      "|   14646.0|{2011-08-11 00:00...|  19150.66|\n",
      "|   12931.0|{2011-08-04 00:00...|  19045.48|\n",
      "|   17949.0|{2011-06-30 00:00...|  18854.78|\n",
      "|   17450.0|{2011-01-11 00:00...|   18620.2|\n",
      "|   14646.0|{2011-02-21 00:00...|  18279.48|\n",
      "|   14646.0|{2011-03-29 00:00...|   18247.5|\n",
      "|   16684.0|{2011-10-05 00:00...|  18047.42|\n",
      "|   14156.0|{2011-01-14 00:00...|  16774.72|\n",
      "|   12415.0|{2011-03-03 00:00...|  16558.14|\n",
      "|   18102.0|{2011-06-09 00:00...|   16488.0|\n",
      "|   14646.0|{2011-05-12 00:00...|  16478.46|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 09:45:33 WARN FileStreamSource: Listed 305 file(s) in 2731 ms          \n",
      "25/05/11 09:45:36 WARN FileStreamSource: Listed 305 file(s) in 2522 ms          \n",
      "25/05/11 09:45:38 WARN FileStreamSource: Listed 305 file(s) in 2417 ms          \n",
      "25/05/11 09:45:41 WARN FileStreamSource: Listed 305 file(s) in 2755 ms          \n",
      "25/05/11 09:45:43 WARN FileStreamSource: Listed 305 file(s) in 2576 ms          \n",
      "25/05/11 09:45:46 WARN FileStreamSource: Listed 305 file(s) in 2683 ms          \n",
      "25/05/11 09:45:49 WARN FileStreamSource: Listed 305 file(s) in 2684 ms          \n",
      "25/05/11 09:45:52 WARN FileStreamSource: Listed 305 file(s) in 2656 ms          \n",
      "25/05/11 09:45:54 WARN FileStreamSource: Listed 305 file(s) in 2490 ms          \n",
      "[Stage 139:=============================>                       (168 + 2) / 305]\r"
     ]
    }
   ],
   "source": [
    "query = purchase_day.writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .queryName(\"customer_purchase\") \\\n",
    "        .outputMode(\"complete\")\n",
    "\n",
    "query.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "316c7e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/11 09:45:55 WARN TaskSetManager: Lost task 172.0 in stage 139.0 (TID 28377) (kumar-rke2-2 executor 1): TaskKilled (Stage cancelled: Job 111 cancelled part of cancelled job group 4d09f35c-48e4-4129-a5f0-e1d5aa4b15ff)\n"
     ]
    }
   ],
   "source": [
    "for stream in spark.streams.active:\n",
    "    stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eac985",
   "metadata": {},
   "source": [
    "### RDD\n",
    "\n",
    "<pre>\n",
    "An RDD can hold any type: ints, strings, dicts, objects — Spark doesn't care at this level. It's just distributed data.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91054e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdd = spark.sparkContext.parallelize(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "77738003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "067664f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3/python/pyspark/sql/session.py:122\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3/python/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3/python/pyspark/sql/session.py:1483\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m-> 1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3/python/pyspark/sql/session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1056\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1058\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3/python/pyspark/sql/session.py:1007\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m   1005\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m samplingRatio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m   1014\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rdd\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[0;32m~/spark-3.5.5-bin-hadoop3/python/pyspark/sql/types.py:1684\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1684\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1685\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1686\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1687\u001b[0m     )\n\u001b[1;32m   1689\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `int`."
     ]
    }
   ],
   "source": [
    "rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44552081",
   "metadata": {},
   "source": [
    "<pre>\n",
    "To convert to a DataFrame, Spark needs structured rows, not just raw elements like 0, 1, 2.\n",
    "\n",
    "That's why this fails:\n",
    "rdd.toDF()  # ❌ PySparkTypeError\n",
    "</pre>\n",
    "#### Fix:\n",
    "<pre>\n",
    "rdd.map(lambda x: (x,)).toDF([\"value\"])  # ✅\n",
    "\n",
    "map(lambda x: (x,)) makes each item a 1-column tuple, like (0,), (1,).\n",
    ".toDF([\"value\"]) assigns a column name.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bec5e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = rdd.map(lambda x: (x,)).toDF([\"value\"])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6a5f2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4fed1bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "schema = StructType([StructField(\"value\", IntegerType(), True)])\n",
    "df1 = spark.createDataFrame(rdd.map(lambda x: [x]), schema)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2929f09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf20e4b",
   "metadata": {},
   "source": [
    "### View Data By partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873eb9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 2, 3, 4]\n",
      "1 [5, 6, 7, 8, 9]\n",
      "2 [10, 11, 12, 13, 14]\n",
      "3 [15, 16, 17, 18, 19]\n",
      "4 [20, 21, 22, 23, 24]\n",
      "5 [25, 26, 27, 28, 29]\n",
      "6 [30, 31, 32, 33, 34]\n",
      "7 [35, 36, 37, 38, 39]\n",
      "8 [40, 41, 42, 43, 44]\n",
      "9 [45, 46, 47, 48, 49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#This will create 10 partitions, allowing Spark to process your data in 10 parallel tasks\n",
    "rdd1 = spark.sparkContext.parallelize(range(50), numSlices=10)\n",
    "\n",
    "def show_partition(index, iterator):\n",
    "    yield index, list(iterator)\n",
    "\n",
    "partioned_data = rdd1.mapPartitionsWithIndex(show_partition)\n",
    "\n",
    "for part in partioned_data.collect():\n",
    "    print(part[0], part[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878612a",
   "metadata": {},
   "source": [
    "🌀 What's a shuffle?\n",
    "\n",
    "A shuffle happens when data needs to move between partitions on different nodes. Common operations that cause this:\n",
    "\n",
    "    groupBy(...)\n",
    "\n",
    "    join(...)\n",
    "\n",
    "    distinct(...)\n",
    "\n",
    "    orderBy(...)\n",
    "\n",
    "    repartition(...)\n",
    "\n",
    "When shuffle happens:\n",
    "\n",
    "    Spark redistributes the data.\n",
    "    The number of output partitions is controlled by spark.sql.shuffle.partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a20b9e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0cc49500",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbcf05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
