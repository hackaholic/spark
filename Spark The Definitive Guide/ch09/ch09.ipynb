{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54429bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bec72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d0d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/15 15:29:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/15 15:29:13 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ch09\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc8e53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kumar-rke2-1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ch09</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=ch09>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1a87d",
   "metadata": {},
   "source": [
    "#### CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f750a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV\n",
    "\n",
    "df_csv1 = spark.read \\\n",
    "     .format(\"csv\") \\\n",
    "     .option(\"mode\", \"FAILFAST\") \\\n",
    "     .option(\"header\", True) \\\n",
    "     .option(\"inferSchema\", True) \\\n",
    "     .option(\"path\", \"hdfs:///data/flight-data/csv/2015-summary.csv\") \\\n",
    "     .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a40635b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e2a0583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Predefine Manual Schema\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "\n",
    "flightSchema = StructType([\n",
    "        StructField('DEST_COUNTRY_NAME', StringType(), True), \n",
    "        StructField('ORIGIN_COUNTRY_NAME', StringType(), True), \n",
    "        StructField('count', IntegerType(), True)\n",
    "     ]) \n",
    "\n",
    "df_csv2 = spark.read \\\n",
    "     .format(\"csv\") \\\n",
    "     .schema(flightSchema) \\\n",
    "     .option(\"mode\", \"FAILFAST\") \\\n",
    "     .option(\"sep\", \",\") \\\n",
    "     .option(\"codec\", \"snappy\") \\\n",
    "     .option(\"header\", True) \\\n",
    "     .load(\"hdfs:///data/flight-data/csv/2015-summary.csv\")\n",
    "\n",
    "df_csv2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6af7903c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68656a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Writing CSV files\n",
    "\n",
    "df_csv2.repartition(5).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"quoteAll\", True) \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .save(\"hdfs:///tmp/flight-data.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3b87c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "-rw-r--r--   2 hadoop supergroup          0 2025-05-15 16:09 /tmp/flight-data.csv/_SUCCESS\n",
      "-rw-r--r--   2 hadoop supergroup       1745 2025-05-15 16:08 /tmp/flight-data.csv/part-00000-bd023d73-9883-4272-b0b8-e3ed86df0708-c000.csv\n",
      "-rw-r--r--   2 hadoop supergroup       1787 2025-05-15 16:08 /tmp/flight-data.csv/part-00001-bd023d73-9883-4272-b0b8-e3ed86df0708-c000.csv\n",
      "-rw-r--r--   2 hadoop supergroup       1761 2025-05-15 16:08 /tmp/flight-data.csv/part-00002-bd023d73-9883-4272-b0b8-e3ed86df0708-c000.csv\n",
      "-rw-r--r--   2 hadoop supergroup       1760 2025-05-15 16:08 /tmp/flight-data.csv/part-00003-bd023d73-9883-4272-b0b8-e3ed86df0708-c000.csv\n",
      "-rw-r--r--   2 hadoop supergroup       1765 2025-05-15 16:09 /tmp/flight-data.csv/part-00004-bd023d73-9883-4272-b0b8-e3ed86df0708-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/flight-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86872d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\",\"count\"\n",
      "\"Greece\",\"United States\",\"30\"\n",
      "\"United States\",\"Bermuda\",\"193\"\n",
      "\"United States\",\"Portugal\",\"134\"\n",
      "\"United States\",\"Trinidad and Tobago\",\"217\"\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /tmp/flight-data.csv/part-00000-414cc8a4-10eb-44d6-bf5b-aa8965565ec6-c000.csv | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05094788",
   "metadata": {},
   "source": [
    "#### Json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13e68a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json1 = spark.read \\\n",
    "           .format(\"json\") \\\n",
    "           .schema(flightSchema) \\\n",
    "           .option(\"mode\", \"FAILFAST\") \\\n",
    "           .load(\"hdfs:///data/flight-data/json/2010-summary.json\")\n",
    "\n",
    "df_json1.show(5)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "625910a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Writing to Json file\n",
    "\n",
    "df_json1.repartition(5).write \\\n",
    "    .format(\"json\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"hdfs:///tmp/flight-data.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5114fc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "-rw-r--r--   2 hadoop supergroup          0 2025-05-15 16:20 /tmp/flight-data.json/_SUCCESS\n",
      "-rw-r--r--   2 hadoop supergroup       4296 2025-05-15 16:20 /tmp/flight-data.json/part-00000-292f6bb8-8b89-4acb-be5e-a36354c881c8-c000.json\n",
      "-rw-r--r--   2 hadoop supergroup       4326 2025-05-15 16:20 /tmp/flight-data.json/part-00001-292f6bb8-8b89-4acb-be5e-a36354c881c8-c000.json\n",
      "-rw-r--r--   2 hadoop supergroup       4238 2025-05-15 16:20 /tmp/flight-data.json/part-00002-292f6bb8-8b89-4acb-be5e-a36354c881c8-c000.json\n",
      "-rw-r--r--   2 hadoop supergroup       4251 2025-05-15 16:20 /tmp/flight-data.json/part-00003-292f6bb8-8b89-4acb-be5e-a36354c881c8-c000.json\n",
      "-rw-r--r--   2 hadoop supergroup       4242 2025-05-15 16:20 /tmp/flight-data.json/part-00004-292f6bb8-8b89-4acb-be5e-a36354c881c8-c000.json\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/flight-data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f238a",
   "metadata": {},
   "source": [
    "#### Parquet Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abef3c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading Parquet\n",
    "\n",
    "df_parqut = spark.read \\\n",
    "            .format(\"parquet\") \\\n",
    "            .option(\"mode\", \"FAILFAST\") \\\n",
    "            .option(\"mergeSchema\", True) \\\n",
    "            .load(\"hdfs:///data/flight-data/parquet/2010-summary.parquet\")\n",
    "\n",
    "df_parqut.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e750fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to Parquet\n",
    "\n",
    "df_parqut.repartition(5).write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"codec\", \"snappy\") \\\n",
    "    .save(\"hdfs:///tmp/flight-data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acf80e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "-rw-r--r--   2 hadoop supergroup          0 2025-05-15 16:36 hdfs:///tmp/flight-data.parquet/_SUCCESS\n",
      "-rw-r--r--   2 hadoop supergroup       2128 2025-05-15 16:36 hdfs:///tmp/flight-data.parquet/part-00000-2c662e19-e737-4fa3-b7a4-efe04ef7edee-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop supergroup       2139 2025-05-15 16:36 hdfs:///tmp/flight-data.parquet/part-00001-2c662e19-e737-4fa3-b7a4-efe04ef7edee-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop supergroup       2075 2025-05-15 16:36 hdfs:///tmp/flight-data.parquet/part-00002-2c662e19-e737-4fa3-b7a4-efe04ef7edee-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop supergroup       2104 2025-05-15 16:36 hdfs:///tmp/flight-data.parquet/part-00003-2c662e19-e737-4fa3-b7a4-efe04ef7edee-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop supergroup       2087 2025-05-15 16:36 hdfs:///tmp/flight-data.parquet/part-00004-2c662e19-e737-4fa3-b7a4-efe04ef7edee-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hdfs:///tmp/flight-data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ebb30",
   "metadata": {},
   "source": [
    "#### ORC File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "792db763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reading ORC\n",
    "\n",
    "df_orc = spark.read \\\n",
    "        .format(\"orc\") \\\n",
    "        .option(\"mode\", \"permissive\") \\\n",
    "        .load(\"hdfs:///data/flight-data/orc/2010-summary.orc\")\n",
    "\n",
    "df_orc.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9019488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Writing to ORC file\n",
    "\n",
    "df_orc.write \\\n",
    "    .format(\"orc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"hdfs:///tmp/flight-data.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8b6f3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 hadoop supergroup          0 2025-05-15 16:46 hdfs:///tmp/flight-data.orc/_SUCCESS\n",
      "-rw-r--r--   2 hadoop supergroup       3929 2025-05-15 16:46 hdfs:///tmp/flight-data.orc/part-00000-f282f10c-f9ac-48d4-a58f-03c625aa12f2-c000.snappy.orc\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hdfs:///tmp/flight-data.orc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf2571",
   "metadata": {},
   "source": [
    "#### SQL Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32038c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8955d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52282d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd49805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688732ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
