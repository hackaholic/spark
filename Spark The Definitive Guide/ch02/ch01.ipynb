{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3dde600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c1305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51391760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/10 14:27:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/10 14:27:54 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# Create the spark session:\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ch01\") \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447db376",
   "metadata": {},
   "outputs": [],
   "source": [
    "myrange = spark.range(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fa4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myrange1 = spark.range(1000).toDF(\"number\") \n",
    "divby2 = myrange1.where(\"number % 2 == 0\")   # transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55264147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0), Row(number=2), Row(number=4)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divby2.take(3)     # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46c383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#12L AS number#14L]\n",
      "+- *(1) Filter ((id#12L % 2) = 0)\n",
      "   +- *(1) Range (0, 1000, step=1, splits=2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "divby2.explain() # the logical transformation plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b8581",
   "metadata": {},
   "source": [
    "### End to End Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightData2015 = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"hdfs:///data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb395e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flightData2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1272bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3425be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "flightData2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79add858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count='15'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count='344'),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count='15'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count='62')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.head(5)     # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98600d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f34cb335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Malta', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Gibraltar', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count='1')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0552bb",
   "metadata": {},
   "source": [
    "### Dataframe and Sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1926d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58eacc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_way = spark.sql(\"\"\"\n",
    "    select DEST_COUNTRY_NAME, count(*) as cnt\n",
    "    from flight_data_2015\n",
    "    group by DEST_COUNTRY_NAME\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efb65fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Anguilla', cnt=1),\n",
       " Row(DEST_COUNTRY_NAME='Russia', cnt=1),\n",
       " Row(DEST_COUNTRY_NAME='Paraguay', cnt=1),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', cnt=1),\n",
       " Row(DEST_COUNTRY_NAME='Sweden', cnt=1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_way.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6bcde31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Anguilla', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Russia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Paraguay', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Sweden', count=1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_way = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
    "data_frame_way.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1a690b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c428d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)='986')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ce63cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)='986')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select max(count)\n",
    "    from flight_data_2015\n",
    "\"\"\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15982e1a",
   "metadata": {},
   "source": [
    "##### Find Top five destination country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "66615ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|DEST_COUNTRY_NAME|   cnt|\n",
      "+-----------------+------+\n",
      "|    United States|411352|\n",
      "|           Canada|  8399|\n",
      "|           Mexico|  7140|\n",
      "|   United Kingdom|  2025|\n",
      "|            Japan|  1548|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Sql way\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select DEST_COUNTRY_NAME, sum(cast(count as int)) as cnt\n",
    "    from flight_data_2015\n",
    "    group by DEST_COUNTRY_NAME\n",
    "    order by cnt desc\n",
    "    limit 5 \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b04be31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|DEST_COUNTRY_NAME|   cnt|\n",
      "+-----------------+------+\n",
      "|    United States|411352|\n",
      "|           Canada|  8399|\n",
      "|           Mexico|  7140|\n",
      "|   United Kingdom|  2025|\n",
      "|            Japan|  1548|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### dataframe Way\n",
    "from pyspark.sql.functions import col, sum, desc\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\") \\\n",
    "    .agg(sum(col(\"count\").cast(\"long\")).alias(\"cnt\")) \\\n",
    "    .sort(desc(\"cnt\")) \\\n",
    "    .limit(5) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "94d0c5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[cnt#292L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#33,cnt#292L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#33], functions=[sum(cast(count#35 as bigint))])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#33, 200), ENSURE_REQUIREMENTS, [plan_id=937]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#33], functions=[partial_sum(cast(count#35 as bigint))])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#33,count#35] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://kumar-rke2-1:9000/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, desc\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\") \\\n",
    "    .agg(sum(col(\"count\").cast(\"long\")).alias(\"cnt\")) \\\n",
    "    .sort(desc(\"cnt\")) \\\n",
    "    .limit(5) \\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a937b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
