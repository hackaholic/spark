{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a282b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64afb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5e3c405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 13:06:07 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession\\\n",
    "        .builder \\\n",
    "        .master(\"yarn\") \\\n",
    "        .appName(\"ch07\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7e466b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kumar-rke2-1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ch07</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=ch07>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b34cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"hdfs:///data/retail-data/all/*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2606f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"retail_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "060bf6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78eb08ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, desc, asc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633de60",
   "metadata": {},
   "source": [
    "#### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c739f51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73f42689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36e51643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select count(*) \n",
    "    from retail_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab6b4ea",
   "metadata": {},
   "source": [
    "#### countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47387b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf15609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "    select count(distinct(StockCode))\n",
    "    from retail_data \n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071de21",
   "metadata": {},
   "source": [
    "#### approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa164f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "492aba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select approx_count_distinct(StockCode, 0.1)\n",
    "    from retail_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73ca54",
   "metadata": {},
   "source": [
    "#### first and last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90e05011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de85995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|           21544|         85049D|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "    select first(StockCode), last(StockCode)\n",
    "    from retail_data\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea2c47",
   "metadata": {},
   "source": [
    "#### min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "583e95da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "377d5e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select min(Quantity), max(Quantity)\n",
    "    from retail_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2492992",
   "metadata": {},
   "source": [
    "#### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "796cbbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42a64965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT SUM(Quantity)\n",
    "    FROM retail_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c9954",
   "metadata": {},
   "source": [
    "#### sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef53446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum_distinct\n",
    "\n",
    "df.select(sum_distinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5146641f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select sum(distinct(Quantity))\n",
    "    from retail_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9635ab9",
   "metadata": {},
   "source": [
    "#### Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "087661a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+----------------+----------------+\n",
      "|count(Quantity)|sum(Quantity)|   avg(Quantity)|   avg(Quantity)|\n",
      "+---------------+-------------+----------------+----------------+\n",
      "|         541909|      5176450|9.55224954743324|9.55224954743324|\n",
      "+---------------+-------------+----------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, expr\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\"),\n",
    "    sum(\"Quantity\"),\n",
    "    avg(\"Quantity\"),\n",
    "    expr(\"avg(Quantity)\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bec355af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+----------------+\n",
      "|count(Quantity)|sum(Quantity)|   avg(Quantity)|\n",
      "+---------------+-------------+----------------+\n",
      "|         541909|      5176450|9.55224954743324|\n",
      "+---------------+-------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select count(Quantity), sum(Quantity),\n",
    "          avg(Quantity)\n",
    "    from retail_data      \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d451224",
   "metadata": {},
   "source": [
    "#### Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47fdb74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+------------------+--------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)| var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+---------------------+------------------+--------------------+\n",
      "| 47559.39140929905|   218.08115785023486|47559.303646609354|  218.08095663447864|\n",
      "+------------------+---------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "df.select(\n",
    "    var_samp(\"Quantity\"),\n",
    "    stddev_samp(\"Quantity\"),\n",
    "    var_pop(\"Quantity\"),\n",
    "    stddev_pop(\"Quantity\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c69df06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+------------------+--------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)| var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+---------------------+------------------+--------------------+\n",
      "| 47559.39140929905|   218.08115785023486|47559.303646609354|  218.08095663447864|\n",
      "+------------------+---------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select var_samp(Quantity),\n",
    "        stddev_samp(Quantity),\n",
    "        var_pop(Quantity),\n",
    "        stddev_pop(Quantity)\n",
    "    from retail_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46aa58",
   "metadata": {},
   "source": [
    "#### skewness and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cd48225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  skewness(Quantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610527843|119768.05495536518|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d4d8462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  skewness(Quantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610527843|119768.05495536518|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select skewness(Quantity), \n",
    "    kurtosis(Quantity)\n",
    "    from retail_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3b238",
   "metadata": {},
   "source": [
    "#### covariance and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a389a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085637639E-4|             1052.7280543913773|            1052.7260778752732|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_samp, covar_pop\n",
    "\n",
    "df.select(\n",
    "    corr(\"InvoiceNo\", \"Quantity\"),\n",
    "    covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "    covar_pop(\"InvoiceNo\", \"Quantity\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "66257d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085637639E-4|             1052.7280543913773|            1052.7260778752732|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        corr(InvoiceNo, Quantity),\n",
    "        covar_samp(InvoiceNo, Quantity),\n",
    "        covar_pop(InvoiceNo, Quantity)\n",
    "    from retail_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b760c",
   "metadata": {},
   "source": [
    "#### Aggregation Complex Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d324aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "\n",
    "df.select(collect_set(\"Country\"), collect_list(\"Country\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63244304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+--------------------+\n",
      "|CustomerID|collect_list(Country)|collect_set(Country)|\n",
      "+----------+---------------------+--------------------+\n",
      "|      NULL| [United Kingdom, ...|[France, Portugal...|\n",
      "|     12346| [United Kingdom, ...|    [United Kingdom]|\n",
      "|     12347| [Iceland, Iceland...|           [Iceland]|\n",
      "+----------+---------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"CustomerID\").agg(\n",
    "    collect_list(\"Country\"),\n",
    "    collect_set(\"Country\") \n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a86c06b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|collect_list(Country)|collect_set(Country)|\n",
      "+---------------------+--------------------+\n",
      "| [United Kingdom, ...|[Portugal, Italy,...|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select  collect_list(Country),\n",
    "    collect_set(Country)\n",
    "    from retail_data \n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bdb8a0",
   "metadata": {},
   "source": [
    "####  groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5903495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 116:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerID|count|\n",
      "+---------+----------+-----+\n",
      "|   563017|     13198|   32|\n",
      "|   563214|     16370|   71|\n",
      "|   563372|     15653|    4|\n",
      "|   563714|     14565|   31|\n",
      "|   564666|     12492|    9|\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerID\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "576cd191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 119:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|InvoiceNo|CustomerID|count(1)|\n",
      "+---------+----------+--------+\n",
      "|   563017|     13198|      32|\n",
      "|   563214|     16370|      71|\n",
      "|   563372|     15653|       4|\n",
      "|   563714|     14565|      31|\n",
      "|   564666|     12492|       9|\n",
      "+---------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select InvoiceNo, CustomerID, count(*)\n",
    "    from retail_data\n",
    "    group by InvoiceNo, CustomerID\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3892562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"), expr(\"stddev_pop(Quantity)\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b2ab273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   563020|16.041666666666668|  10.212244148841895|\n",
      "|   565747|              10.3|   3.163858403911275|\n",
      "|   566248|               9.0|   8.703447592764606|\n",
      "|   566431| 14.11111111111111|   6.911254017815104|\n",
      "|   567163|              14.5|  11.280514172678478|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select InvoiceNo, avg(Quantity), stddev_pop(Quantity)\n",
    "    from retail_data\n",
    "    group by InvoiceNo\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da83302",
   "metadata": {},
   "source": [
    "#### Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a92eb649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number, first_value, last_value\n",
    "from pyspark.sql.functions import lag, lead, desc_nulls_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a1273442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 140:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|CustomerID|row_number|\n",
      "+----------+----------+\n",
      "|     12346|         1|\n",
      "|     12346|         2|\n",
      "|     12347|         1|\n",
      "|     12347|         2|\n",
      "|     12347|         3|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.where(col(\"CustomerID\").isNotNull()).select(\"CustomerID\",\n",
    "    (row_number().over(Window.partitionBy(\"CustomerID\").orderBy(desc_nulls_last(\"InvoiceNo\"))).alias(\"row_number\"))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bce23026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 146:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|CustomerId|row_number|\n",
      "+----------+----------+\n",
      "|     12346|         1|\n",
      "|     12346|         2|\n",
      "|     12347|         1|\n",
      "|     12347|         2|\n",
      "|     12347|         3|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select CustomerId,\n",
    "          row_number() over(partition by CustomerID order by InvoiceNo) as row_number\n",
    "    from retail_data\n",
    "    where CustomerID is not null\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e0f71efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "datedf = df.withColumn(\"date\", to_date(\"InvoiceDate\", \"MM/d/yyyy H:mm\"))\n",
    "datedf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8199bb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datedf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "73f76115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('InvoiceNo', 'string'),\n",
       " ('StockCode', 'string'),\n",
       " ('Description', 'string'),\n",
       " ('Quantity', 'int'),\n",
       " ('InvoiceDate', 'string'),\n",
       " ('UnitPrice', 'double'),\n",
       " ('CustomerID', 'int'),\n",
       " ('Country', 'string'),\n",
       " ('date', 'date')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datedf.select(\"*\").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cd3e2621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 227:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------------+----+----------+\n",
      "|CustomerID|       Country|max_purchase|rank|dense_rank|\n",
      "+----------+--------------+------------+----+----------+\n",
      "|     12346|United Kingdom|       74215|   1|         1|\n",
      "|     12346|United Kingdom|       74215|   2|         2|\n",
      "|     12347|       Iceland|         240|   1|         1|\n",
      "|     12347|       Iceland|         240|   2|         2|\n",
      "|     12347|       Iceland|         240|   3|         3|\n",
      "+----------+--------------+------------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"CustomerID\", \"Country\") \\\n",
    "    .orderBy(desc(\"Quantity\")) \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "datedf.where(\"CustomerID is not NULL and  Quantity IS NOT NULL\") \\\n",
    "      .orderBy(\"CustomerID\") \\\n",
    "      .select(\n",
    "          \"CustomerID\", \"Country\",\n",
    "          (max(\"Quantity\").over(windowSpec)).alias(\"max_purchase\"),\n",
    "          (rank().over(windowSpec)).alias(\"rank\"),\n",
    "          (dense_rank().over(windowSpec)).alias(\"dense_rank\")\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fa521aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 250:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------+-------+---+-----+\n",
      "|CustomerID|       Country|Quantity|max_qty|rnk|d_rnk|\n",
      "+----------+--------------+--------+-------+---+-----+\n",
      "|     18287|United Kingdom|       4|      4|  1|    1|\n",
      "|     18287|United Kingdom|       4|      4|  1|    1|\n",
      "|     18287|United Kingdom|       4|      4|  1|    1|\n",
      "|     18287|United Kingdom|       4|      4|  1|    1|\n",
      "|     18287|United Kingdom|       6|      6|  5|    2|\n",
      "|     18287|United Kingdom|       6|      6|  5|    2|\n",
      "|     18287|United Kingdom|       6|      6|  5|    2|\n",
      "|     18287|United Kingdom|       6|      6|  5|    2|\n",
      "|     18287|United Kingdom|       6|      6|  5|    2|\n",
      "|     18287|United Kingdom|       6|      6|  5|    2|\n",
      "+----------+--------------+--------+-------+---+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select CustomerID, Country, Quantity,\n",
    "    max(Quantity) over(partition by CustomerID, Country order by Quantity ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as max_qty,\n",
    "    rank(Quantity) over(partition by CustomerID, Country order by Quantity ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as rnk,\n",
    "    dense_rank(Quantity) over(partition by CustomerID, Country order by Quantity ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)as d_rnk\n",
    "    from retail_data\n",
    "    order by CustomerID desc nulls last, rnk\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa327eb6",
   "metadata": {},
   "source": [
    "#### Grouping Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dcde1e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 252:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+\n",
      "|CustomerID|       Country|sum(Quantity)|\n",
      "+----------+--------------+-------------+\n",
      "|     12346|United Kingdom|            0|\n",
      "|     12347|       Iceland|         2458|\n",
      "|     12348|       Finland|         2341|\n",
      "|     12349|         Italy|          631|\n",
      "|     12350|        Norway|          197|\n",
      "+----------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.filter(\"CustomerID is not null and Country is not NULL\") \\\n",
    "    .groupBy(\"CustomerID\", \"Country\") \\\n",
    "    .sum(\"Quantity\") \\\n",
    "    .orderBy(\"CustomerID\") \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4e4db9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 258:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+\n",
      "|CustomerID|       Country|sum(Quantity)|\n",
      "+----------+--------------+-------------+\n",
      "|     18287|United Kingdom|         1586|\n",
      "|     18283|United Kingdom|         1397|\n",
      "|     18282|United Kingdom|           98|\n",
      "|     18281|United Kingdom|           54|\n",
      "|     18280|United Kingdom|           45|\n",
      "+----------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select CustomerID, Country, sum(Quantity)\n",
    "    from retail_data\n",
    "    group by CustomerID, Country\n",
    "    order by CustomerID desc NULLS LAST\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af2521",
   "metadata": {},
   "source": [
    "#### Rollup and cube\n",
    "\n",
    "For Understanding Lets do it on small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6451f560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| id|  x|  y|\n",
      "+---+---+---+\n",
      "|  a|foo|  1|\n",
      "|  a|foo|  2|\n",
      "|  a|bar|  2|\n",
      "|  a|bar|  2|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small = spark.createDataFrame(\n",
    "    [(\"a\", \"foo\", 1),\n",
    "     (\"a\", \"foo\", 2),\n",
    "     (\"a\", \"bar\", 2),\n",
    "     (\"a\", \"bar\", 2)],\n",
    "    [\"id\", \"x\", \"y\"])\n",
    "df_small.createOrReplaceTempView(\"df_small\")\n",
    "\n",
    "df_small.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3813a90",
   "metadata": {},
   "source": [
    "<pre>\n",
    "cube(\"id\", \"x\", \"y\") will return (), (id), (x), (y), (id, x), (id, y), (x, y), (id, x, y).\n",
    "(All the possible grouping existent combinations.)\n",
    "\n",
    "rollup(\"id\", \"x\", \"y\") will only return \n",
    "() ->  no grouping \n",
    "(id) -> Grouping on (id)\n",
    "(id, x), -> grouping on (id,x)\n",
    "(id, x, y) -> grouping on (id, x ,y)\n",
    "(Combinations which include the beginning of the provided sequence.)\n",
    "\n",
    "groupBy(\"id\", \"x\", \"y\") will only return (id, x, y) combination.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9c8b83cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 263:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|  id|   x|   y|count|\n",
      "+----+----+----+-----+\n",
      "|NULL|NULL|NULL|    4|\n",
      "|   a|NULL|NULL|    4|\n",
      "|   a| bar|NULL|    2|\n",
      "|   a| bar|   2|    2|\n",
      "|   a| foo|NULL|    2|\n",
      "|   a| foo|   1|    1|\n",
      "|   a| foo|   2|    1|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# rollup\n",
    "df_small.rollup(\"id\", \"x\", \"y\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c8efb297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|  id|   x|   y|count|\n",
      "+----+----+----+-----+\n",
      "|NULL|NULL|   2|    3|\n",
      "|NULL|NULL|NULL|    4|\n",
      "|   a|NULL|   2|    3|\n",
      "|   a| foo|NULL|    2|\n",
      "|   a| foo|   1|    1|\n",
      "|   a|NULL|   1|    1|\n",
      "|NULL| foo|NULL|    2|\n",
      "|   a|NULL|NULL|    4|\n",
      "|NULL|NULL|   1|    1|\n",
      "|NULL| foo|   2|    1|\n",
      "|NULL| foo|   1|    1|\n",
      "|   a| foo|   2|    1|\n",
      "|NULL| bar|NULL|    2|\n",
      "|NULL| bar|   2|    2|\n",
      "|   a| bar|NULL|    2|\n",
      "|   a| bar|   2|    2|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.cube(\"id\", \"x\", \"y\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9eb60702",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13a37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
